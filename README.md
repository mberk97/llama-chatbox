# LLaMA 3.1 Chatbox API (FastAPI)

This project demonstrates a lightweight chatbox service powered by the LLaMA 3.1 language model. It exposes the model via a RESTful API using **FastAPI**, serving as a proof-of-concept for basic AI integration and backend development.

---

## ğŸ“Œ Assignment Purpose

This project showcases the ability to:

- Integrate a LLaMA-based model in Python
- Develop and deploy a RESTful API with FastAPI
- Validate functionality using Postman

---

## âš™ï¸ Technologies Used

| Component       | Technology            |
|----------------|------------------------|
| Model Backend   | LLaMA 3.1 (Python)     |
| API Framework   | FastAPI (Python)       |
| Testing Tool    | Postman                |

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py             # FastAPI app and routing
â”‚   â””â”€â”€ model.py            # LLaMA model loading and inference
â”œâ”€â”€ models/
â”‚   â””â”€â”€ LLaMA-3-8B.gguf     # Model file (not tracked in repo)
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ postman-demo.png    # Postman test example
â”‚   â”œâ”€â”€ swagger-ui.png      # Swagger UI screenshot
â”‚   â””â”€â”€ server-log.png      # Server running confirmation
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## ğŸ”Œ API: `/chat` Endpoint

**Method**: `POST`  
**URL**: `http://127.0.0.1:8000/chat`

### âœ… Request Body (JSON)

```json
{
  "message": "Hello!"
}
```

### ğŸ“¤ Sample Response (JSON)

```json
{
  "reply": "Hello! How can I help you today?"
}
```

---

## ğŸ–¼ï¸ Screenshots

| Description              | File Path                    |
|--------------------------|------------------------------|
| Postman request          | `screenshots/postman-demo.png` |
| Swagger UI (FastAPI Docs) | `screenshots/swagger-ui.png`  |
| Server Startup Console   | `screenshots/server-log.png`  |

To include them visually in your README:

```markdown
![Postman](./screenshots/postman-demo.png)
![Swagger UI](./screenshots/swagger-ui.png)
![Server Log](./screenshots/server-log.png)
```

---

## ğŸ’¡ Technical Notes

Although **FastAPI** was used for rapid prototyping, the architecture can be modularized for production-readiness.

### ğŸ”„ Scalable Architecture Plan

- **Inference Layer (Python)**: LLaMA 3.1 remains in Python for efficient model execution.
- **API Gateway (Java Spring Boot)**: Ideal for enterprise applications with microservice support.
- **Communication**: REST or gRPC used between Java (API) and Python (Model).
- **Benefits**: Clear separation of concerns, microservice readiness, language flexibility.

---

## ğŸš€ Future Enhancements

- Use **WebSockets** for real-time message streaming
- Add **React or Vue** frontend for a full-stack chat interface
- Implement **multi-turn memory** or context handling with session-based storage
- Add **Docker support** for containerization

---

## âœ… Conclusion

This proof-of-concept demonstrates basic integration of a local LLM using FastAPI. For future development, I aim to evolve this into a modular microservice-based architecture using Spring Boot (API) and Python (inference), enabling scalable, maintainable, and production-grade AI applications.